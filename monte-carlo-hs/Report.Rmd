---
title: Parallelism in Haskell
subtitle: Parallel computing
author: Meelis Utt
date: 2020-11-11
header-includes:
  - \usepackage{hyperref}
  - \usepackage{amssymb}
output: pdf_document
---


\newcommand{\refr}[1]{
(\ref{#1})
}

```{r,include=F,echo=F}
# load necessary package
library(ggplot2)
library(dplyr)
library(parallel)
```

```{r,include=F}
knitr::opts_chunk$set(fig.width = 5, fig.height = 3)
```

\section*{Introduction}
For this report I chose Haskell programming language, as I am currently very interested in it.
Haskell is stronly general-purpose, strongly typed, lazily evaluated purely functional programming language $\refr{realworkhaskell1}$.
Haskell separates pure functions and functions with side efects.
Pure function is a function, that gives same output for same input every time, meaning it is deterministic $\refr{wikipure}$.
Side efect can be printing to standard output, sending data over network, generating random number etc.
It started in the academia, meaning scientist were the ones who mostly developed it in the beginning.
In Haskell there is two types of parallelism: pure parallelism and concurrency $\refr{wikipar}$.
I also found packages (haskell-mpi,mpi-hs), that use MPI for parallelization.
I found some mentions of OpenMP in Haskell, but I did not currently research this.
I planned on using MPI solution at first, but I had a dependecy issue, that I was unable to not resolve.
Since the example of Monte-Carlo solution I made for this report is more aligned with pure parallelism, then I will be focusing more on this.
Although, reading the materials gave me an impression, that concurrency is really useful in networking $\refr{realworkhaskell24}$.
I would like to share link to a video about concurrency in haskell, that I found really useful and interesting $\refr{concurvid}$.
\newline
I used Haskell package called parallel, which is part of the pure parallelism I mentioned before.
Pure parallelism has the advantages of
\begin{itemize}
  \item Guaranteed deterministic (same result every time);
  \item no race conditions or deadlocks $\refr{wikipar}$.
\end{itemize}
Since Haskell is lazy language and has immutable variables, then it is possible to take a normal Haskell function, apply a few simple transformations to it and have it evaluated in parallel $\refr{realworkhaskell24}$.
This video $\refr{parvid}$ helped me understand this subject and gave some examples of good and bad parallelism.
When making my parallel implementation of this Monte-Carlo example, I had a lot of trouble of getting the implementation reasonable in a way that gave me speedup. 
In the package \textit{parallel} the parallelism is handled by the runtime system (RTS).
Parallel work is done by \textit{spark}'s.
Spark is something that takes some unevaluated data and evaluates it in parallel.
When a spark is created, it is put into the \textit{spark pool}, which is the Haskell Execution Contect (HEC).
One HEC is roughly equivalent to one core on ones machine.
For more indepth overview of sparks, I once again recommend this $\refr{parvid}$ video.
Before moving on to the examples, I want to mention that there is a greate tool $\refr{sparktool}$ for spark event analysis, that I unfortunatly did not use during this example.
Also, I want to mention that since the package is currently marked (at the time of writing) experimental, then there exists possibility that some functions may change.
However, since the package is fairly popular, then there is small probability for a major change.
\section*{Example}
First let's look at the setup of the example.
In this example, we want to find the mean value of the function
\[
  f(x) = x^2 + x^4 + \sin(x) + \cos(x) + x^{25}.
\]
This function was chosen, because it is fairly simple to find the mean analytically, but it still gives some computational complexity when using the Monte-Carlo method.
The mean can be calculate analytically using the formula
\[
  E(f(x)) = \int_{a}^{b} f(x)dx.
\]
In my example, I used uniform distribution $X\sim U(0,1)$ to generate the random values in the Monte-Carlo method.
So the analytically we get
\[
  E(f(x)) = \int_{0}^{1} f(x)dx 
    = \int_{0}^{1} x^2 + x^4 + \sin(x) + \cos(x) + x^25 dx
    = \frac{613}{390} + \sin(1) - \cos(1) \approx 1.8729635507346285.
\]


I implemented the function, analytical solution, generator of uniform distribution values and timing functinos separately from the MC examples and imported the compiled code to serial and parallel examples.
At first I parallelised the calculation of mean.
Later I also tried to parallelize the generation of random numbers.
However, I did not succeed in getting very good parallelization.
I compile both serial and parallel code with command
\begin{verbatim}
stack ghc -- -threaded -rtsopts -eventlog -main-is MC<type> MC<type>.hs
\end{verbatim}
and the executable can be run with
\begin{verbatim}
./MC<type> <n> +RTS -N
or
./MC<type> <n> +RTS -N -s
\end{verbatim}
In case of serial code, the flags \textit{+RTS} and \textit{-N} do nothing,
but for the sake of comparability I added them.
In case of parallel program, the flag \textit{-N} specifies how many cores are used.
If there is no number specified in the flag (eg -N2), then the maximum number of cores are used.
The \textit{-s} option gives more info about the running of the program and 
\textit{-ls} would create a file, that could be used in the spark analysis tool I mentioned.
For example, the flag \textit{-s} gives us output

```{sh}
./MCserial 100 +RTS -N -s
```
```{sh}
./MCparallel 100 +RTS -N -s
```

As we can see from this example, the codes ran roughly at the same speed (although the error on parallel program might be bit more than $0.05$ in some cases).
However, in Monte-Carlo method it is usual to run the code with bigger $n$.
Let's try running the code with $n = 10^3,5\cdot10^3,10^4,5\cdot10^4,\dotso,10^7$.
We get

```{sh}
rm -f results.csv
for n in 1000 5000 10000 50000 100000 1000000 5000000 10000000
do
  ./MCserial $n +RTS -N >> results.csv
  ./MCparallel $n +RTS -N >> results.csv
done
```

```{r}
data <- read.csv("results.csv",header=F,stringsAsFactors=F) %>%
  setNames(c("n","result","analytical","error","type","time"))
data
ggplot(data=data, aes(x=n,y=time,color=type,group=type)) +
  geom_point() +
  geom_line() +
  labs(
       title="Serial vs parallel",
       x="Nr of iterations",
       y="Walltime (sec)"
       ) +
  theme(
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)
       ) +
  guides(color=guide_legend(title="Type"))
```

The parallel solution is faster in case of all the $n$ values.
For smaller $n$ values it is difficult to see the difference on the plot.
Let's look at the relative speedup.

```{r}
par <- data[data$type=="parallel",]
ser <- data[data$type=="serial",]
relative <- dplyr::inner_join(par,ser,by="n") %>% 
  mutate(
    type = "relative speedup",
    relative.speedup = time.y/time.x
  ) %>% 
  select(n,type,relative.speedup)
relative
ggplot(data=relative, aes(x=n,y=relative.speedup,color=type,group=type)) +
  geom_point() +
  geom_line() +
  labs(
       title="Serial vs parallel",
       x="Nr of iterations",
       y="Relative speedup"
       ) +
  theme(
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)
       ) +
  guides(color=guide_legend(title="Type"))
```

As we can see, the speedup converges arount $1.8$.
The best speedup we gained was `r relative[relative$relative.speedup == max(relative$relative.speedup),"relative.speedup"]`.\
I did not achieve very good speedup with this example.
I think it is possible to get better speedups, if generation of random numbers is parallelized better.
Furthermore, I think that generation of random numbers is done in an inefficient why.
Unfortunately, I was unable to improve the generation.
To put into perspective, I put together a simple serial and parallel MC method implementation in R.
Since R is dynamic language, it should be slower.
Let's compare the relative speedups of the quick serial and parallel R implementation against this Haskell example.
We get

```{r}
f <- function(x){
  return(x**2 + x**4+ sin(x) + cos(x) +x**25)
}
analytical <- 613/390 + sin(1) - cos(1)
powers <- 3:7
iterations <- rep(c(1,5),each=length(powers))*10**(powers)
iterations <- iterations[-length(iterations)]
# serial
null <- lapply(iterations,function(n){
    start <- Sys.time()
    i <- runif(n,0,1)
    EX <- mean(f(i))
    end <- Sys.time()
    time <- difftime(end,start)
    error <- EX - analytical
    write.table(file = "results.csv",
                x = paste(n,EX,analytical,error,"R serial",time,sep = ","),
                append = TRUE,col.names=F,row.names=F,quote=F)
  }
)

library(parallel)
#parallel
null <- sapply(iterations,function(n){
  start <- Sys.time()
  # Calculate the number of cores
  no_cores <- detectCores()
  start <- Sys.time()
  
  # Initiate cluster
  cl <- makeCluster(no_cores)
  
  intermean <- parSapply(cl, rep(n/no_cores,no_cores),function(n,f){
    i <- runif(n,0,1)
    EX <- mean(f(i))
    
  },f
  )
  stopCluster(cl)
  EX <- mean(intermean)
  end <- Sys.time()
  time <- difftime(end,start)
  error <- EX - analytical
    write.table(file = "results.csv",
                x = paste(n,EX,analytical,error,"R parallel",time,sep = ","),
                append = TRUE,col.names=F,row.names=F,quote=F)
})
```

```{r}
data <- read.csv("results.csv",header=F,stringsAsFactors=F) %>%
  setNames(c("n","result","analytical","error","type","time"))
data %>% arrange(n,type)

par <- data[data$type=="parallel",]
Rser <- data[data$type=="R serial",]
Rpar <- data[data$type=="R parallel",]

parRser <- dplyr::inner_join(Rser,par,by="n") %>%
  mutate(
    type = "Haskell parallel vs R serial",
    relative.speedup = time.y/time.x
  ) %>% 
  select(n,type,relative.speedup)

parRpar <- dplyr::inner_join(Rpar,par,by="n") %>%
  mutate(
    type = "Haskell parallel vs R parallel",
    relative.speedup = time.y/time.x
  ) %>% 
  select(n,type,relative.speedup)

RparRser <- dplyr::inner_join(Rpar,Rser,by="n") %>%
  mutate(
    type = "serial vs parallel R",
    relative.speedup = time.y/time.x
  ) %>% 
  select(n,type,relative.speedup)

relative <- dplyr::union_all(parRser,parRpar) %>%
  dplyr::union_all(RparRser)
relative

ggplot(data=parRser, aes(x=n,y=relative.speedup,color=type,group=type)) +
  geom_point() +
  geom_line() +
  labs(
       title="Parallel Haskell vs serial R",
       x="Nr of iterations",
       y="Relative speedup"
       ) +
  theme(
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)
       ) +
  guides(color=guide_legend(title="Type"))

ggplot(data=parRpar, aes(x=n,y=relative.speedup,color=type,group=type)) +
  geom_point() +
  geom_line() +
  labs(
       title="Parallel Haskell vs R",
       x="Nr of iterations",
       y="Relative speedup"
       ) +
  theme(
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)
       ) +
  guides(color=guide_legend(title="Type"))

ggplot(data=RparRser, aes(x=n,y=relative.speedup,color=type,group=type)) +
  geom_point() +
  geom_line() +
  labs(
       title="Serial vs parallel R",
       x="Nr of iterations",
       y="Relative speedup"
       ) +
  theme(
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)
       ) +
  guides(color=guide_legend(title="Type"))


```

As we can see, serial R example has better speedup, when $n$ was smaller.
However, when $n$ got bigger, the parallel implementation was faster.

\newpage
\section*{Used literature}
\begin{enumerate}
  \item \url{http://book.realworldhaskell.org/}, 2020-11-11\label{realworkhaskell1}
  \item \url{https://wiki.haskell.org/Parallelism}, 2020-11-11\label{wikipar}
  \item \url{http://book.realworldhaskell.org/read/concurrent-and-multicore-programming.html}, 2020-11-11\label{realworkhaskell24}
  \item \url{https://wiki.haskell.org/Pure}, 2020-11-11\label{wikipure}
  \item \url{https://www.youtube.com/watch?v=cuHD2qTXxL4}, 2020-11-11\label{concurvid}
  \item \url{https://www.youtube.com/watch?v=R47959rD2yw}, 2020-11-11\label{parvid}
  \item \url{https://wiki.haskell.org/ThreadScope_Tour/Spark}, 2020-11-11\label{sparktool}
\end{enumerate}

